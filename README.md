# Evaluating the Effectiveness of Stochastic Gradient Descent in Neural Network Optimization



A project comparing Stochastic Gradient Descent (SGD) and standard Gradient Descent (GD) on a neural network trained to approximate a complex non-linear function. The comparison focuses on performance metrics like test loss and computation time across multiple training runs.

## Abstract

A common problem in Machine Learning is overfitting, especially when training data is limited. In this paper, we explore the effectiveness of stochastic gradient descent compared to classical gradient descent on performance of neural networks, when optimizing
for a complex non-linear function approximation. Using a simple neural network we compare the two methods using identical training conditions. Our results shows that Stochastic Gradient Descent not only leads to lower test loss but also reduces overall computation time.

### Weights & Biases

The experiment results can be seen on Weights & Biases:
[Weights & Biases Results](https://wandb.ai/peterdallhansen-danmarks-tekniske-universitet-dtu/dtu-sgd-gd-comparison)

### Installation

To install the required packages, you can use pip:

```bash
pip install -r requirements.txt
```
