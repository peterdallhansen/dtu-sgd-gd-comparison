{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e06e73b",
   "metadata": {},
   "source": [
    "# Statistical Comparison: SGD vs GD\n",
    "\n",
    "This notebook compares the performance of **Stochastic Gradient Descent (SGD)** and **Gradient Descent (GD)** on a neural network regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47880aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea59734",
   "metadata": {},
   "source": [
    "## Create Challenge Dataset\n",
    "DO NOT CHANGE THE CODE THAT CREATES THE DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weird function\n",
    "def weird_fun(x):\n",
    "    return np.sin(1 / x)\n",
    "\n",
    "\n",
    "# Reset random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Set data parameters\n",
    "N = 50  # Number of observations\n",
    "s = 0.02  # Noise standard deviation\n",
    "N_test = 1000  # Number of test observations\n",
    "\n",
    "# Create training set\n",
    "x_train = np.sort(np.random.rand(N) * 2 - 1)\n",
    "y_train = weird_fun(x_train) + s * np.random.randn(N)\n",
    "\n",
    "# Create test set\n",
    "x_test = np.sort(np.random.rand(N_test) * 2 - 1)\n",
    "y_test = weird_fun(x_test) + s * np.random.randn(N_test)\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(x_train, y_train, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f73786",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the hyperparameters, that both models will use\n",
    "H = 64  # Hidden units\n",
    "learning_rate = 0.001\n",
    "activation = \"Tanh\" # We choose to use TanH as it will give a smoother curve\n",
    "max_epochs = 10_000\n",
    "num_layers = 3\n",
    "batch_size = 1\n",
    "\n",
    "# Device to use for computations\n",
    "device = torch.device(\"cpu\") # As i am on macbook i will just use cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563894f",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6797c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Here i choose to create the model as a function so i easily can create two models\n",
    "def create_model(H, num_layers, activation):\n",
    "    activation_fn = getattr(torch.nn, activation)()\n",
    "\n",
    "    layers = []\n",
    "    layers.append(torch.nn.Linear(1, H))\n",
    "    layers.append(activation_fn)\n",
    "\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(torch.nn.Linear(H, H))\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "    layers.append(torch.nn.Linear(H, 1))\n",
    "\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0f2fa",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    H,\n",
    "    learning_rate,\n",
    "    num_layers,\n",
    "    activation,\n",
    "    max_epochs,\n",
    "    device,\n",
    "    batch_size=None,   \n",
    "):\n",
    "    import time\n",
    "    import torch\n",
    "\n",
    "    # create tensors\n",
    "    x = torch.tensor(np.expand_dims(x_train, 1), dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(np.expand_dims(y_train, 1), dtype=torch.float32, device=device)\n",
    "    x_test_t = torch.tensor(np.expand_dims(x_test, 1), dtype=torch.float32, device=device)\n",
    "    y_test_t = torch.tensor(np.expand_dims(y_test, 1), dtype=torch.float32, device=device)\n",
    "\n",
    "    n_samples = len(x)\n",
    "\n",
    "    # default batch size\n",
    "    if batch_size is None:\n",
    "        batch_size = n_samples\n",
    "\n",
    "    # create model\n",
    "    model = create_model(H, num_layers, activation).to(device)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # create dataloader\n",
    "    dataset = torch.utils.data.TensorDataset(x, y)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(batch_size < n_samples), # Only shuffle if we are using batch size smaller than dataset\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    Loss_train = []\n",
    "    Loss_test = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        # training\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            y_pred = model(xb)\n",
    "            loss = loss_fn(y_pred, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        Loss_train.append(avg_loss)\n",
    "\n",
    "        # test\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(x_test_t)\n",
    "            test_loss = loss_fn(test_pred, y_test_t).item()\n",
    "        Loss_test.append(test_loss)\n",
    "\n",
    "      \n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"train_loss\": Loss_train,\n",
    "        \"test_loss\": Loss_test,\n",
    "        \"final_train_loss\": Loss_train[-1],\n",
    "        \"final_test_loss\": Loss_test[-1],\n",
    "        \"computation_time\": computation_time,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e1e53",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fb7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GD  - Final Test Loss: 0.272030\n",
      "  SGD - Final Test Loss: 0.218844\n",
      "\n",
      "Trial 2/10\n",
      "  Training with GD...\n",
      "  Training with SGD...\n",
      "  GD  - Final Test Loss: 0.271578\n",
      "  SGD - Final Test Loss: 0.230712\n",
      "\n",
      "Trial 3/10\n",
      "  Training with GD...\n",
      "  Training with SGD...\n",
      "  GD  - Final Test Loss: 0.270488\n",
      "  SGD - Final Test Loss: 0.226214\n",
      "\n",
      "Trial 4/10\n",
      "  Training with GD...\n",
      "  Training with SGD...\n"
     ]
    }
   ],
   "source": [
    "# Number of runs\n",
    "n_trials = 10\n",
    "\n",
    "gd_results = []\n",
    "sgd_results = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"\\nTrial {trial + 1}/{n_trials}\")\n",
    "\n",
    "    torch.manual_seed(42 + trial)\n",
    "    np.random.seed(42 + trial)\n",
    "\n",
    "    # Train with GD\n",
    "    print(\"  Training with GD...\")\n",
    "    gd_result = train(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        H,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        activation,\n",
    "        max_epochs,\n",
    "        device,\n",
    "        batch_size=None,\n",
    "    )\n",
    "    gd_results.append(gd_result)\n",
    "\n",
    "    # Reset random seed again for fair comparison\n",
    "    torch.manual_seed(42 + trial)\n",
    "    np.random.seed(42 + trial)\n",
    "\n",
    "    # Train with SGD\n",
    "    print(\"  Training with SGD...\")\n",
    "    sgd_result = train(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        H,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        activation,\n",
    "        max_epochs,\n",
    "        device,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    sgd_results.append(sgd_result)\n",
    "\n",
    "    print(f\"  GD  - Final Test Loss: {gd_result['final_test_loss']:.6f}\")\n",
    "    print(f\"  SGD - Final Test Loss: {sgd_result['final_test_loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd11d9",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4380b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_final_test = [r[\"final_test_loss\"] for r in gd_results]\n",
    "sgd_final_test = [r[\"final_test_loss\"] for r in sgd_results]\n",
    "\n",
    "gd_times = [r[\"computation_time\"] for r in gd_results]\n",
    "sgd_times = [r[\"computation_time\"] for r in sgd_results]\n",
    "\n",
    "# Mean test loss\n",
    "gd_mean_loss = np.mean(gd_final_test)\n",
    "sgd_mean_loss = np.mean(sgd_final_test)\n",
    "\n",
    "print(\"Mean Test Loss:\")\n",
    "print(f\"  GD:  {gd_mean_loss:.6f}\")\n",
    "print(f\"  SGD: {sgd_mean_loss:.6f}\")\n",
    "\n",
    "\n",
    "def ci_95(data):\n",
    "    mean = np.mean(data)\n",
    "    sem = stats.sem(data)\n",
    "    return stats.t.interval(0.95, df=len(data) - 1, loc=mean, scale=sem)\n",
    "\n",
    "# 95% confidence intervals for test loss\n",
    "gd_ci_low, gd_ci_high = ci_95(gd_final_test)\n",
    "sgd_ci_low, sgd_ci_high = ci_95(sgd_final_test)\n",
    "\n",
    "print(\"\\n95% Confidence Interval (Test Loss):\")\n",
    "print(f\"  GD:  [{gd_ci_low:.6f}, {gd_ci_high:.6f}]\")\n",
    "print(f\"  SGD: [{sgd_ci_low:.6f}, {sgd_ci_high:.6f}]\")\n",
    "\n",
    "\n",
    "# Mean computation time\n",
    "gd_mean_time = np.mean(gd_times)\n",
    "sgd_mean_time = np.mean(sgd_times)\n",
    "\n",
    "print(\"\\nMean Computation Time:\")\n",
    "print(f\"  GD:  {gd_mean_time:.3f} seconds\")\n",
    "print(f\"  SGD: {sgd_mean_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4dac9",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb211cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract test losses across trials\n",
    "gd = np.array([r[\"test_loss\"] for r in gd_results])\n",
    "sgd = np.array([r[\"test_loss\"] for r in sgd_results])\n",
    "epochs = np.arange(gd.shape[1])\n",
    "\n",
    "\n",
    "def mean_ci(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0, ddof=1)\n",
    "    se = std / np.sqrt(data.shape[0])\n",
    "    ci95 = 1.96 * se\n",
    "    return mean, ci95\n",
    "\n",
    "\n",
    "gd_mean, gd_ci = mean_ci(gd)\n",
    "sgd_mean, sgd_ci = mean_ci(sgd)\n",
    "\n",
    "\n",
    "# Downsample for plotting (keeps clarity)\n",
    "idx = np.linspace(0, len(epochs) - 1, 400).astype(int)\n",
    "\n",
    "# PGFPlots colors\n",
    "COLOR_GD = \"#1f77b4\"\n",
    "COLOR_SGD = \"#ff7f0e\"\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"mathtext.fontset\": \"cm\",\n",
    "        \"axes.edgecolor\": \"black\",\n",
    "        \"axes.linewidth\": 0.8,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "        \"grid.color\": \"gray\",\n",
    "        \"grid.linewidth\": 0.5,\n",
    "        \"grid.alpha\": 0.3,\n",
    "        \"legend.frameon\": True,\n",
    "        \"legend.edgecolor\": \"black\",\n",
    "        \"legend.fancybox\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4.8))\n",
    "\n",
    "# Shaded CI regions\n",
    "ax.fill_between(\n",
    "    epochs[idx],\n",
    "    gd_mean[idx] - gd_ci[idx],\n",
    "    gd_mean[idx] + gd_ci[idx],\n",
    "    color=COLOR_GD,\n",
    "    alpha=0.15,\n",
    ")\n",
    "ax.fill_between(\n",
    "    epochs[idx],\n",
    "    sgd_mean[idx] - sgd_ci[idx],\n",
    "    sgd_mean[idx] + sgd_ci[idx],\n",
    "    color=COLOR_SGD,\n",
    "    alpha=0.15,\n",
    ")\n",
    "\n",
    "# Mean curves\n",
    "ax.plot(\n",
    "    epochs[idx], gd_mean[idx], color=COLOR_GD, linewidth=2, label=\"GD Mean ± 95% CI\"\n",
    ")\n",
    "ax.plot(\n",
    "    epochs[idx],\n",
    "    sgd_mean[idx],\n",
    "    color=COLOR_SGD,\n",
    "    linewidth=2,\n",
    "    label=\"SGD Mean ± 95% CI\",\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Test Loss\")\n",
    "ax.set_title(\"Mean Test Loss over Epochs (with 95% CI)\", fontsize=16)\n",
    "\n",
    "# Move legend below title\n",
    "ax.legend(loc=\"upper center\", ncol=2, frameon=True)\n",
    "\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
